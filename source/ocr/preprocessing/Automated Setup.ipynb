{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Pre-Processing Model Setup\n",
    "\n",
    "> This notebook works well with the `Python 3 (Data Science)` kernel on SageMaker Studio\n",
    "\n",
    "In this notebook, we'll show how you can use AWS SDKs to automatically set up a Rekognition Custom Labels model from the provided sample dataset.\n",
    "\n",
    "For an alternative manual walkthrough, see the [README.md](README.md) in this same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation\n",
    "\n",
    "First, in the cell below, we'll:\n",
    "\n",
    "- **Import** the libraries we'll use in this notebook\n",
    "- **Connect** to AWS services via the SDKs\n",
    "- **Configure** our environment\n",
    "\n",
    "You'll need to fill in the `PreprocessTrainingBucketName` created by your solution stack. You can find this from the **Outputs tab** of your particular stack, selected from the list in the [CloudFormation console](https://console.aws.amazon.com/cloudformation/home?#/stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # The general-purpose AWS SDK for Python\n",
    "import sagemaker  # Additional higher-level APIs for SageMaker\n",
    "\n",
    "rekognition = boto3.client(\"rekognition\")\n",
    "\n",
    "training_bucket_name = # TODO: something like \"stack-name-preprocesstrainingbucket-abc123456\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch the Labelled Data\n",
    "\n",
    "The sample data is publicly available via Amazon S3 - with images already classified into 'good' and 'bad' sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P data -N https://public-asean-textract-demo-ap-southeast-1.s3-ap-southeast-1.amazonaws.com/receipts.zip\n",
    "\n",
    "with ZipFile(\"data/receipts.zip\", \"r\") as zip_ref:\n",
    "    print(\"Unzipping...\")\n",
    "    zip_ref.extractall(\"data\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload to Amazon S3\n",
    "\n",
    "To use with Rekognition Custom Labels, we'll load the decompressed images into Amazon S3 in the same AWS Region and Account that our solution is deployed in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --quiet ./data s3://$training_bucket_name/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Manifest File\n",
    "\n",
    "Our images have already been categorized into folders, so there's no need to manually re-label them using either [Amazon SageMaker Ground Truth](https://aws.amazon.com/sagemaker/groundtruth/) or the Rekognition Custom Labels console.\n",
    "\n",
    "Instead, we'll create a **manifest file** for our dataset as described [in the Rekognition developer guide](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/cd-manifest-files-classification.html) - listing out each image and the corresponding annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/receipts.manifest.jsonl\", \"w\") as fmanifest:\n",
    "    for class_ix, class_name in enumerate((\"bad\", \"good\")):\n",
    "        meta = {\n",
    "            \"class-name\": class_name,\n",
    "            \"confidence\": 0.0,\n",
    "            \"type\": \"groundtruth/image-classification\",\n",
    "            \"job-name\": \"does-not-exist\",\n",
    "            \"human-annotated\": \"yes\",\n",
    "            \"creation-date\": \"2021-06-01T00:00:00.000000\"\n",
    "        }\n",
    "        for filename in os.listdir(os.path.join(\"data\", class_name)):\n",
    "            fmanifest.write(json.dumps({\n",
    "                \"source-ref\": f\"s3://{training_bucket_name}/{class_name}/{filename}\",\n",
    "                \"label\": class_ix,\n",
    "                \"label-metadata\": meta,\n",
    "            }) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this manifest itself will need to be loaded to Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_s3uri = f\"s3://{training_bucket_name}/receipts.manifest.jsonl\"\n",
    "\n",
    "!aws s3 cp data/receipts.manifest.jsonl $manifest_s3uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start Rekognition Custom Labels Training\n",
    "\n",
    "With the annotated dataset now ready on Amazon S3 in a compatible format, we can create a **Project** in Rekognition, and start the process of training a model version (\"project version\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"receipts\"\n",
    "\n",
    "print(f\"Creating Rekognition Custom Labels project '{project_name}'...\")\n",
    "\n",
    "create_project_resp = rekognition.create_project(\n",
    "    ProjectName=project_name,\n",
    ")\n",
    "project_arn = create_project_resp[\"ProjectArn\"]\n",
    "create_project_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rek_asset = {\n",
    "    \"GroundTruthManifest\": {\n",
    "        \"S3Object\": {\n",
    "            \"Bucket\": training_bucket_name,\n",
    "            \"Name\": manifest_s3uri[len(\"s3://\"):].partition(\"/\")[2],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "version_name = f\"{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "print(f\"Starting model training for version '{version_name}'...\")\n",
    "create_project_version_resp = rekognition.create_project_version(\n",
    "    ProjectArn=project_arn,\n",
    "    VersionName=version_name,\n",
    "    OutputConfig={\n",
    "        \"S3Bucket\": training_bucket_name,\n",
    "        \"S3KeyPrefix\": f\"rekognition/{project_name}\",\n",
    "    },\n",
    "    TrainingData={\n",
    "        \"Assets\": [dataset_rek_asset]\n",
    "    },\n",
    "    TestingData={\n",
    "        'Assets': [dataset_rek_asset],\n",
    "    },\n",
    ")\n",
    "\n",
    "project_version_arn = create_project_version_resp[\"ProjectVersionArn\"]\n",
    "create_project_version_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Your model version ARN:\\n{project_version_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Waiting and Model Deployment\n",
    "\n",
    "The above step kicked off version training in the background - which will take some time to complete.\n",
    "\n",
    "You can check the status in the Rekognition Custom Labels console, or instead wait for completion via boto3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rekognition.get_waiter(\"project_version_training_completed\").wait(\n",
    "    ProjectArn=project_arn,\n",
    "    VersionNames=[\n",
    "        version_name,\n",
    "    ],\n",
    "    WaiterConfig={\n",
    "        \"Delay\": 60,  # in seconds\n",
    "        \"MaxAttempts\": 60 * 60 * 2,\n",
    "    },\n",
    ")\n",
    "print(\"Project version training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rekognition.describe_project_versions(\n",
    "    ProjectArn=project_arn,\n",
    "    VersionNames=[\n",
    "        version_name,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your model is trained, you can **connect it to your solution stack** as follows:\n",
    "\n",
    "- In the [AWS SSM Parameter Store](https://console.aws.amazon.com/systems-manager/parameters/?&tab=Table) console, find the deployed stack's `RekognitionModelArn` parameter.\n",
    "- **Edit** your parameter to set the *Value* as your model version ARN as displayed above.\n",
    "\n",
    "This model is trained, but not yet deployed. At the moment, the solution Lambda will trigger deployment when first invoked - but still fail until deployment is complete. Let's also trigger deployment from here in the notebook, to avoid first calls failing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rekognition.start_project_version(\n",
    "    ProjectVersionArn=project_version_arn,\n",
    "    MinInferenceUnits=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with training, this is an asynchronous operation and we have the option to wait until it's complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rekognition.get_waiter(\"project_version_running\").wait(\n",
    "    ProjectArn=project_arn,\n",
    "    VersionNames=[\n",
    "        version_name,\n",
    "    ],\n",
    "    WaiterConfig={\n",
    "        \"Delay\": 30,  # in seconds\n",
    "        \"MaxAttempts\": 40,\n",
    "    },\n",
    ")\n",
    "print(\"Model deployed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "\n",
    "In Rekognition Custom Labels, inference [pricing](https://aws.amazon.com/rekognition/pricing/) is by deployed capacity - not processed requests... So when you're done experimenting with your solution - be sure to 'stop' the project version to avoid unnecessary charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rekognition.stop_project_version(\n",
    "    ProjectVersionArn=project_version_arn,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
